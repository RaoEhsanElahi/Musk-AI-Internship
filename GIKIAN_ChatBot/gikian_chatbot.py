# -*- coding: utf-8 -*-
"""GIKIAN_ChatBot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oH8iBAIavziM5q8KlBN9aF5UqJ7xJIRK

# MY_CHATBOT for GIKIANS to be

##Loading URLs, ScrapeData
"""

!pip install langchain
!pip install huggingface_hub
!pip install sentence_transformers
!pip install faiss-cpu
!pip install xformers
!pip install chromadb

"""### Get HUGGINGFACEHUB_API_KEY"""

import os
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "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"

"""##Upload Text File containing URLs"""

def read_urls_from_file(file_path):
    urls_list = []
    try:
        with open(file_path, 'r') as file:
            for line in file:
                # Assuming each line contains a single URL
                url = line.strip()  # Remove any leading/trailing whitespace or newline characters
                urls_list.append(url)
    except FileNotFoundError:
        print("Error: The file was not found.")
    except Exception as e:
        print(f"Error: An unexpected error occurred: {e}")

    return urls_list

# Replace 'file_path.txt' with the actual path of your file containing URLs.
file_path = './clean.txt'
urls_list = read_urls_from_file(file_path)

# Print the list of URLs
print(urls_list)

"""##Loading Scrape Data."""

from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter

with open('./cleanScrape.txt') as f:
    documents = f.read()

text_splitter = RecursiveCharacterTextSplitter(
    # Set a really small chunk size, just to show.
    chunk_size = 1000,
    chunk_overlap  = 0,
    length_function = len,  )

texts = text_splitter.create_documents([documents])

len(texts)

"""##Pre-Processing"""

import textwrap

def wrap_text_preserve_newlines(text, width=110):
    # Split the input text into lines based on newline characters
    lines = text.split('\n')

    # Wrap each line individually
    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]

    # Join the wrapped lines back together using newline characters
    wrapped_text = '\n'.join(wrapped_lines)

    return wrapped_text

print(wrap_text_preserve_newlines(str(texts[0])))

"""### Embeddings"""

# Embeddings
from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings()

from langchain.vectorstores import FAISS

db = FAISS.from_documents(texts, embeddings)
#If embeddings are stored in drive.
#db = FAISS.load_local("/content/drive/MyDrive/db_index", embeddings)

"""## Loading Model."""

from langchain.chains.question_answering import load_qa_chain
from langchain.chains.qa_with_sources import load_qa_with_sources_chain
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain import HuggingFaceHub

llm = HuggingFaceHub(repo_id="tiiuae/falcon-7b", model_kwargs={"temperature":0.1, "max_length":256})

chain = load_qa_chain(llm, chain_type="stuff")

query = "What are admission criteria in GIKI?"
docs = db.similarity_search(query)
chain.run(input_documents=docs, question=query)

query = "When and where did the giki Entry test helds?"
docs = db.similarity_search(query)
chain.run(input_documents=docs, question=query)

"""### Working with DB"""

from langchain.indexes import VectorstoreIndexCreator
from langchain.chains import RetrievalQA
import sqlite3

# connect your Google Drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

db_path = '/content/gdrive/My Drive/db_llm/GIKIAN.db'
conn = sqlite3.connect(db_path)

#execute SQL commands
cursor = conn.cursor()

def insert_into_database(query, response):
    cursor.execute('INSERT INTO chatbot_history (query, chatbot_response) VALUES (?, ?)', (query, response))
    conn.commit()

def get_chatbot_response(question, docs):
    response = chain.run(input_documents=docs, question=question)
    return response

def get_similar_documents(query):
    docs = db.similarity_search(query)
    return docs

if __name__ == "__main__":
    while True:
      user_input = input("Enter your query (type 'exit' to quit): ").strip().lower()
      if user_input == 'exit':
        print("\nThankYou for Your time. Please Come again!\nExiting...")
        break
      relevant_docs = get_similar_documents(user_input)
      chatbot_response = get_chatbot_response(user_input, relevant_docs)
      insert_into_database(user_input, chatbot_response)

      print("Chatbot: ", chatbot_response)

"""#Vector Memory."""

index = VectorstoreIndexCreator(
    embedding=HuggingFaceEmbeddings(),
    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders("loaders")

llm=HuggingFaceHub(repo_id="bigscience/bloom-1b7", model_kwargs={"temperature":0, "max_length":1024})

chain = RetrievalQA.from_chain_type(llm=llm,
                                    chain_type="stuff",
                                    retriever=index.vectorstore.as_retriever(),
                                    input_key="question")

chain.run('How is the Giki as an institute?')

chain.run('Who are the Administration of GIKI')

chain.run("How old is GIKI's infrastructure?")

